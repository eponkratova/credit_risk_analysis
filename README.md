---
title: "Credit Risk Analysis & Models explainability with LIME"
author: "EPonkratova"
date: "February 02, 2019"
---

“Why Should I Trust You?" Explaining the Predictions of Any Classifier" by Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin which could be accessed at https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf
In the paper, a group of authors proposes Local Interpretable Model-agnostic Explanations (LIME), a technique to explain the predictions of any classifier or regressor by learning an interpretable model locally around the prediction and Submodular pick for explaining models (SP-LIME), a method for building trust in the predictions of a model overall. The paper is organized as follows: An introduction provides a brief overview of trust; 'The case for explanations' discusses the concept of explainability; 'Local interpretable model-agnostic explanations' introduces the method to interpret the predictions of complex models and illustrates it with real examples; Submodular pick for explaining models offers an algorithm for picking a set of non-redundant instances derived from instances (xs) in order to give global understanding of the reviewed black box model; Simulated user experience presents ‘simulated user experiments to evaluate the utility of explanations in trust-related tasks'; Evaluation with Human Subjects evaluates LIME and SP-LIME; and Related Work.
The authors start with the analysis of the concept of trust, examining the distinction between trusting a model and trusting predictions, 'if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed.'
Explainability of predictions, defined as presenting visual or textual artifacts which allow mapping inputs to their outputs is a key to winning trust in algorithms. Review of individual predictions will not only help with the acceptance of the predictions but also provide global understanding of the model by identifying variables/relationship that are not working as expected and compare different models in addition to metrics typically used to compare model performance. An interpretable model explainer as authors suggest, should contain the following properties:
- interpretability defined as the ability to explain or to present model results in understandable terms to a human.
- local fidelity meaning that a prediction produced by an explanation should agree with the original model as much as possible.
- to be model agnostic i.e. should be able to explain any model without making any assumptions about model while providing explanations.
- to provide a global perspective i.e. give a global intuition of the model to a user.
To achieve local fidelity, an instance x is selected and a set of new data points in the neighborhood of x along with the corresponding predictions are created 'uniformly at random (where the number of such draws is also uniformly sampled)'. Then, these permutated samples are weighted by their closeness to the instance of interest, and a linear model is trained on this weighted dataset. The parameters of the linear model will help to explain the predictions for the selected record x.
The explanation model for the instance x is the model that minimizes the degree L(f, g, πx) of 'how unfaithful g is in approximating f in the locality defined' while keeping the model complexity Ω(g) low, e.g. by selecting the maximum number of features.
The main critique of LIME was offered by the authors themselves stating that even if a learned linear model appears to be good approximation to the predictions locally i.e. in the neighborhood of the prediction of the permutated samples, it doesn't have to be a good global approximation i.e. will not explain the model for all data points as the neighborhood can be too small or too uniform. Ribeiro M., Singh S., and Guestrin C. tried to tackle it in the paper by proposing Sub modular pick 'to give a global understanding of the model by picking a set of' diverse but representative instances which will allow returning a non-redundant explanation set. The idea of the 'pick step' is to train LIME on all instances i.e. all x. After constructing an n d d explanation matrix W where n represents instances and d represents features, compute the global importance of every feature in the explanation space. Then, add the instance with the highest maximum coverage gain to get a representative non-redundant explanation set V.
Regardless whether a random pick or submodular pick is used, the LIME explanations require domain knowledge and expertise which will enable practitioners to use the models in making data-based decisions, rather than to rely on the data science generalists.
